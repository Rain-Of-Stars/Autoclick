# -*- coding: utf-8 -*-
"""
ä»£ç è§„èŒƒåŒ–åˆ†æå·¥å…·ï¼šæ£€æŸ¥ä»£ç é£æ ¼ã€å¯¼å…¥é¡ºåºã€æ³¨é‡Šè§„èŒƒç­‰
"""

import os
import ast
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import Set, Dict, List, Tuple, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent


class CodeStandardizationAnalyzer:
    """ä»£ç è§„èŒƒåŒ–åˆ†æå™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.issues = defaultdict(list)  # é—®é¢˜åˆ†ç±»
        self.statistics = defaultdict(int)  # ç»Ÿè®¡ä¿¡æ¯
        
        # ç¼–ç è§„èŒƒ
        self.encoding_patterns = [
            r'# -\*- coding: utf-8 -\*-',
            r'# coding: utf-8',
            r'# coding=utf-8'
        ]
        
        # å¯¼å…¥é¡ºåºè§„èŒƒï¼ˆPEP 8ï¼‰
        self.import_order = [
            'standard',    # æ ‡å‡†åº“
            'third_party', # ç¬¬ä¸‰æ–¹åº“
            'local'        # æœ¬åœ°æ¨¡å—
        ]
        
        # æ ‡å‡†åº“æ¨¡å—
        self.standard_modules = {
            'os', 'sys', 'time', 'datetime', 'json', 'pickle', 'logging',
            'threading', 'multiprocessing', 'queue', 'collections', 'typing',
            'pathlib', 'shutil', 'subprocess', 'signal', 'warnings', 'ctypes',
            'ast', 're', 'uuid', 'traceback', 'hashlib', 'dataclasses',
            'functools', 'itertools', 'operator', 'math', 'random', 'string'
        }
        
        # ç¬¬ä¸‰æ–¹åº“æ¨¡å—
        self.third_party_modules = {
            'PySide6', 'cv2', 'numpy', 'PIL', 'psutil', 'requests', 
            'aiohttp', 'websockets', 'qasync'
        }
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        relative_path = file_path.relative_to(self.project_root)
        file_issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
            
            # æ£€æŸ¥ç¼–ç å£°æ˜
            encoding_issues = self._check_encoding(lines, str(relative_path))
            file_issues.extend(encoding_issues)
            
            # æ£€æŸ¥å¯¼å…¥é¡ºåº
            import_issues = self._check_import_order(content, str(relative_path))
            file_issues.extend(import_issues)
            
            # æ£€æŸ¥æ³¨é‡Šè§„èŒƒ
            comment_issues = self._check_comments(lines, str(relative_path))
            file_issues.extend(comment_issues)
            
            # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
            docstring_issues = self._check_docstrings(content, str(relative_path))
            file_issues.extend(docstring_issues)
            
            # æ£€æŸ¥ä»£ç é£æ ¼
            style_issues = self._check_code_style(lines, str(relative_path))
            file_issues.extend(style_issues)
            
            return {
                'file_path': str(relative_path),
                'issues': file_issues,
                'issue_count': len(file_issues)
            }
            
        except Exception as e:
            error_issue = {
                'type': 'file_error',
                'severity': 'error',
                'message': f"æ— æ³•åˆ†ææ–‡ä»¶: {e}",
                'line': 0
            }
            return {
                'file_path': str(relative_path),
                'issues': [error_issue],
                'issue_count': 1
            }
    
    def _check_encoding(self, lines: List[str], file_path: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ç¼–ç å£°æ˜"""
        issues = []
        
        # æ£€æŸ¥å‰ä¸¤è¡Œæ˜¯å¦æœ‰ç¼–ç å£°æ˜
        has_encoding = False
        for i, line in enumerate(lines[:2]):
            for pattern in self.encoding_patterns:
                if re.search(pattern, line):
                    has_encoding = True
                    break
            if has_encoding:
                break
        
        if not has_encoding:
            issues.append({
                'type': 'encoding_missing',
                'severity': 'warning',
                'message': 'ç¼ºå°‘UTF-8ç¼–ç å£°æ˜',
                'line': 1,
                'suggestion': 'åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ : # -*- coding: utf-8 -*-'
            })
        
        return issues
    
    def _check_import_order(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥å¯¼å…¥é¡ºåºï¼ˆPEP 8ï¼‰"""
        issues = []
        
        try:
            tree = ast.parse(content)
            imports = []
            
            # æå–æ‰€æœ‰å¯¼å…¥è¯­å¥
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            module_name = alias.name.split('.')[0]
                            imports.append({
                                'line': node.lineno,
                                'module': module_name,
                                'type': self._classify_import(module_name),
                                'statement': f"import {alias.name}"
                            })
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module_name = node.module.split('.')[0]
                            imports.append({
                                'line': node.lineno,
                                'module': module_name,
                                'type': self._classify_import(module_name),
                                'statement': f"from {node.module} import ..."
                            })
            
            # æ£€æŸ¥å¯¼å…¥é¡ºåº
            if len(imports) > 1:
                current_type = None
                for imp in imports:
                    if current_type is None:
                        current_type = imp['type']
                    elif self._get_import_order_index(imp['type']) < self._get_import_order_index(current_type):
                        issues.append({
                            'type': 'import_order',
                            'severity': 'info',
                            'message': f"å¯¼å…¥é¡ºåºä¸ç¬¦åˆPEP 8è§„èŒƒ: {imp['statement']}",
                            'line': imp['line'],
                            'suggestion': 'æŒ‰ç…§æ ‡å‡†åº“ã€ç¬¬ä¸‰æ–¹åº“ã€æœ¬åœ°æ¨¡å—çš„é¡ºåºæ’åˆ—å¯¼å…¥'
                        })
                    current_type = imp['type']
        
        except SyntaxError:
            pass  # è¯­æ³•é”™è¯¯ç”±å…¶ä»–å·¥å…·å¤„ç†
        
        return issues
    
    def _classify_import(self, module_name: str) -> str:
        """åˆ†ç±»å¯¼å…¥æ¨¡å—"""
        if module_name in self.standard_modules:
            return 'standard'
        elif module_name in self.third_party_modules:
            return 'third_party'
        elif module_name in ['auto_approve', 'workers', 'capture', 'utils', 'tests', 'tools']:
            return 'local'
        else:
            # ç®€å•åˆ¤æ–­ï¼šå°å†™ä¸”ä¸å«ä¸‹åˆ’çº¿çš„å¯èƒ½æ˜¯ç¬¬ä¸‰æ–¹åº“
            if module_name.islower() and '_' not in module_name and len(module_name) > 2:
                return 'third_party'
            return 'local'
    
    def _get_import_order_index(self, import_type: str) -> int:
        """è·å–å¯¼å…¥ç±»å‹çš„é¡ºåºç´¢å¼•"""
        try:
            return self.import_order.index(import_type)
        except ValueError:
            return len(self.import_order)
    
    def _check_comments(self, lines: List[str], file_path: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ³¨é‡Šè§„èŒƒ"""
        issues = []
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # æ£€æŸ¥ä¸­æ–‡æ³¨é‡Šè§„èŒƒ
            if '#' in line and not stripped.startswith('#'):
                comment_part = line[line.index('#'):]
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
                if re.search(r'[\u4e00-\u9fff]', comment_part):
                    # æ£€æŸ¥æ³¨é‡Šå‰æ˜¯å¦æœ‰ç©ºæ ¼
                    if not re.search(r'\s+#', line):
                        issues.append({
                            'type': 'comment_spacing',
                            'severity': 'info',
                            'message': 'è¡Œå†…æ³¨é‡Šå‰åº”æœ‰ä¸¤ä¸ªç©ºæ ¼',
                            'line': i,
                            'suggestion': 'åœ¨#å‰æ·»åŠ ä¸¤ä¸ªç©ºæ ¼'
                        })
            
            # æ£€æŸ¥TODO/FIXMEæ³¨é‡Š
            if re.search(r'#\s*(TODO|FIXME|XXX|HACK)', stripped, re.IGNORECASE):
                issues.append({
                    'type': 'todo_comment',
                    'severity': 'info',
                    'message': 'å‘ç°å¾…åŠäº‹é¡¹æ³¨é‡Š',
                    'line': i,
                    'suggestion': 'è€ƒè™‘åˆ›å»ºissueè·Ÿè¸ªæ­¤é¡¹ç›®'
                })
        
        return issues
    
    def _check_docstrings(self, content: str, file_path: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²"""
        issues = []
        
        try:
            tree = ast.parse(content)
            
            # æ£€æŸ¥æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²
            if not ast.get_docstring(tree):
                issues.append({
                    'type': 'module_docstring_missing',
                    'severity': 'warning',
                    'message': 'ç¼ºå°‘æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²',
                    'line': 1,
                    'suggestion': 'åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ¨¡å—è¯´æ˜æ–‡æ¡£'
                })
            
            # æ£€æŸ¥å‡½æ•°å’Œç±»çš„æ–‡æ¡£å­—ç¬¦ä¸²
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if not ast.get_docstring(node) and not node.name.startswith('_'):
                        issues.append({
                            'type': 'function_docstring_missing',
                            'severity': 'info',
                            'message': f'å…¬å…±å‡½æ•° {node.name} ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²',
                            'line': node.lineno,
                            'suggestion': 'ä¸ºå…¬å…±å‡½æ•°æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜å‚æ•°å’Œè¿”å›å€¼'
                        })
                
                elif isinstance(node, ast.ClassDef):
                    if not ast.get_docstring(node):
                        issues.append({
                            'type': 'class_docstring_missing',
                            'severity': 'info',
                            'message': f'ç±» {node.name} ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²',
                            'line': node.lineno,
                            'suggestion': 'ä¸ºç±»æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²è¯´æ˜å…¶ç”¨é€”'
                        })
        
        except SyntaxError:
            pass
        
        return issues
    
    def _check_code_style(self, lines: List[str], file_path: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä»£ç é£æ ¼"""
        issues = []
        
        for i, line in enumerate(lines, 1):
            # æ£€æŸ¥è¡Œé•¿åº¦
            if len(line) > 120:
                issues.append({
                    'type': 'line_too_long',
                    'severity': 'info',
                    'message': f'è¡Œé•¿åº¦è¶…è¿‡120å­—ç¬¦ ({len(line)}å­—ç¬¦)',
                    'line': i,
                    'suggestion': 'å°†é•¿è¡Œæ‹†åˆ†ä¸ºå¤šè¡Œ'
                })
            
            # æ£€æŸ¥å°¾éšç©ºæ ¼
            if line.rstrip() != line and line.strip():
                issues.append({
                    'type': 'trailing_whitespace',
                    'severity': 'info',
                    'message': 'è¡Œå°¾æœ‰å¤šä½™ç©ºæ ¼',
                    'line': i,
                    'suggestion': 'åˆ é™¤è¡Œå°¾ç©ºæ ¼'
                })
            
            # æ£€æŸ¥åˆ¶è¡¨ç¬¦
            if '\t' in line:
                issues.append({
                    'type': 'tab_character',
                    'severity': 'warning',
                    'message': 'ä½¿ç”¨äº†åˆ¶è¡¨ç¬¦ï¼Œåº”ä½¿ç”¨ç©ºæ ¼',
                    'line': i,
                    'suggestion': 'å°†åˆ¶è¡¨ç¬¦æ›¿æ¢ä¸º4ä¸ªç©ºæ ¼'
                })
        
        return issues
    
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        python_files = list(self.project_root.rglob('*.py'))
        
        print(f"åˆ†æ {len(python_files)} ä¸ªPythonæ–‡ä»¶çš„ä»£ç è§„èŒƒ...")
        
        all_issues = []
        file_results = []
        
        for file_path in python_files:
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜
            if any(part in str(file_path) for part in ['.venv', '__pycache__', '.git']):
                continue
            
            result = self.analyze_file(file_path)
            file_results.append(result)
            all_issues.extend(result['issues'])
        
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        issue_types = Counter(issue['type'] for issue in all_issues)
        severity_counts = Counter(issue['severity'] for issue in all_issues)
        
        return {
            'total_files': len(file_results),
            'total_issues': len(all_issues),
            'issue_types': dict(issue_types),
            'severity_counts': dict(severity_counts),
            'file_results': file_results
        }
    
    def generate_report(self) -> str:
        """ç”Ÿæˆä»£ç è§„èŒƒåŒ–æŠ¥å‘Š"""
        analysis = self.analyze_project()
        
        report = []
        report.append("=" * 60)
        report.append("ä»£ç è§„èŒƒåŒ–åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        # æ€»ä½“ç»Ÿè®¡
        report.append(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        report.append(f"  - åˆ†ææ–‡ä»¶æ•°: {analysis['total_files']}")
        report.append(f"  - å‘ç°é—®é¢˜æ•°: {analysis['total_issues']}")
        
        # é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        report.append(f"\nğŸš¨ é—®é¢˜ä¸¥é‡ç¨‹åº¦:")
        for severity, count in analysis['severity_counts'].items():
            emoji = {'error': 'âŒ', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}.get(severity, 'â€¢')
            report.append(f"  {emoji} {severity}: {count}")
        
        # é—®é¢˜ç±»å‹åˆ†å¸ƒ
        report.append(f"\nğŸ“‹ é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for issue_type, count in sorted(analysis['issue_types'].items(), key=lambda x: x[1], reverse=True):
            report.append(f"  - {issue_type}: {count}")
        
        # é—®é¢˜æœ€å¤šçš„æ–‡ä»¶
        report.append(f"\nğŸ“ é—®é¢˜æœ€å¤šçš„æ–‡ä»¶:")
        file_issues = [(result['file_path'], result['issue_count']) 
                      for result in analysis['file_results']]
        file_issues.sort(key=lambda x: x[1], reverse=True)
        
        for file_path, issue_count in file_issues[:5]:
            if issue_count > 0:
                report.append(f"  - {file_path}: {issue_count} ä¸ªé—®é¢˜")
        
        # æ”¹è¿›å»ºè®®
        report.append(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        if analysis['issue_types'].get('encoding_missing', 0) > 0:
            report.append("  â€¢ ä¸ºæ‰€æœ‰Pythonæ–‡ä»¶æ·»åŠ UTF-8ç¼–ç å£°æ˜")
        
        if analysis['issue_types'].get('import_order', 0) > 0:
            report.append("  â€¢ æŒ‰ç…§PEP 8è§„èŒƒæ•´ç†å¯¼å…¥é¡ºåº")
        
        if analysis['issue_types'].get('module_docstring_missing', 0) > 0:
            report.append("  â€¢ ä¸ºæ¨¡å—æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²")
        
        if analysis['issue_types'].get('line_too_long', 0) > 0:
            report.append("  â€¢ å°†è¿‡é•¿çš„ä»£ç è¡Œæ‹†åˆ†")
        
        if analysis['issue_types'].get('trailing_whitespace', 0) > 0:
            report.append("  â€¢ æ¸…ç†è¡Œå°¾å¤šä½™ç©ºæ ¼")
        
        return '\n'.join(report)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = CodeStandardizationAnalyzer(project_root)
    report = analyzer.generate_report()
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = project_root / 'tests' / 'code_standardization_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


if __name__ == "__main__":
    main()
