# -*- coding: utf-8 -*-
"""
ä¾èµ–åˆ†æå·¥å…·ï¼šæ£€æŸ¥é¡¹ç›®ä¸­å®é™…ä½¿ç”¨çš„ä¾èµ–åŒ…
"""

import os
import sys
import ast
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import Set, Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class DependencyAnalyzer:
    """ä¾èµ–åˆ†æå™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.imports_found = defaultdict(set)  # æ–‡ä»¶ -> å¯¼å…¥é›†åˆ
        self.third_party_imports = Counter()  # ç¬¬ä¸‰æ–¹åº“è®¡æ•°
        self.standard_library_imports = Counter()  # æ ‡å‡†åº“è®¡æ•°
        self.local_imports = Counter()  # æœ¬åœ°æ¨¡å—è®¡æ•°
        
        # Pythonæ ‡å‡†åº“æ¨¡å—ï¼ˆéƒ¨åˆ†å¸¸ç”¨çš„ï¼‰
        self.standard_modules = {
            'os', 'sys', 'time', 'datetime', 'json', 'pickle', 'logging',
            'threading', 'multiprocessing', 'queue', 'collections', 'typing',
            'pathlib', 'shutil', 'subprocess', 'signal', 'warnings', 'ctypes',
            'ast', 're', 'uuid', 'traceback', 'hashlib', 'dataclasses',
            'functools', 'itertools', 'operator', 'math', 'random', 'string',
            'io', 'tempfile', 'glob', 'fnmatch', 'csv', 'configparser',
            'urllib', 'http', 'email', 'base64', 'zlib', 'gzip', 'tarfile',
            'zipfile', 'sqlite3', 'xml', 'html', 'unittest', 'doctest'
        }
        
        # å·²çŸ¥ç¬¬ä¸‰æ–¹åº“æ˜ å°„ï¼ˆåŒ…å -> requirementsä¸­çš„åç§°ï¼‰
        self.package_mapping = {
            'cv2': 'opencv-python',
            'numpy': 'numpy', 
            'np': 'numpy',
            'PIL': 'Pillow',
            'PySide6': 'PySide6',
            'psutil': 'psutil',
            'requests': 'requests',
            'aiohttp': 'aiohttp',
            'websockets': 'websockets',
            'qasync': 'qasync',
            'windows_capture': 'windows-capture'
        }
    
    def analyze_file(self, file_path: Path) -> Set[str]:
        """åˆ†æå•ä¸ªPythonæ–‡ä»¶çš„å¯¼å…¥"""
        imports = set()
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä½¿ç”¨ASTè§£æ
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            imports.add(alias.name.split('.')[0])
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports.add(node.module.split('.')[0])
            except SyntaxError:
                # ASTè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
                import_patterns = [
                    r'^\s*import\s+([a-zA-Z_][a-zA-Z0-9_]*)',
                    r'^\s*from\s+([a-zA-Z_][a-zA-Z0-9_]*)\s+import'
                ]
                
                for line in content.split('\n'):
                    for pattern in import_patterns:
                        match = re.match(pattern, line)
                        if match:
                            imports.add(match.group(1))
        
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")
        
        return imports
    
    def categorize_import(self, import_name: str) -> str:
        """åˆ†ç±»å¯¼å…¥ï¼šstandard/third_party/local"""
        if import_name in self.standard_modules:
            return 'standard'
        elif import_name in self.package_mapping or import_name in ['cv2', 'numpy', 'PIL', 'PySide6', 'psutil', 'requests', 'aiohttp', 'websockets', 'qasync']:
            return 'third_party'
        elif import_name in ['auto_approve', 'workers', 'capture', 'utils', 'tests', 'tools']:
            return 'local'
        else:
            # å°è¯•åˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸‰æ–¹åº“
            if import_name.islower() and '_' not in import_name and len(import_name) > 2:
                return 'third_party'
            return 'local'
    
    def analyze_project(self) -> Dict:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        python_files = list(self.project_root.rglob('*.py'))
        
        print(f"åˆ†æ {len(python_files)} ä¸ªPythonæ–‡ä»¶...")
        
        for file_path in python_files:
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜
            if any(part in str(file_path) for part in ['.venv', '__pycache__', '.git']):
                continue
            
            imports = self.analyze_file(file_path)
            relative_path = file_path.relative_to(self.project_root)
            self.imports_found[str(relative_path)] = imports
            
            # åˆ†ç±»ç»Ÿè®¡
            for imp in imports:
                category = self.categorize_import(imp)
                if category == 'standard':
                    self.standard_library_imports[imp] += 1
                elif category == 'third_party':
                    self.third_party_imports[imp] += 1
                else:
                    self.local_imports[imp] += 1
        
        return {
            'files_analyzed': len(python_files),
            'total_imports': sum(len(imports) for imports in self.imports_found.values()),
            'third_party': dict(self.third_party_imports),
            'standard': dict(self.standard_library_imports),
            'local': dict(self.local_imports)
        }
    
    def check_requirements_consistency(self) -> Dict:
        """æ£€æŸ¥requirements.txtä¸å®é™…ä½¿ç”¨çš„ä¸€è‡´æ€§"""
        requirements_file = self.project_root / 'requirements.txt'
        
        if not requirements_file.exists():
            return {'error': 'requirements.txt ä¸å­˜åœ¨'}
        
        # è¯»å–requirements.txt
        declared_packages = set()
        with open(requirements_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æå–åŒ…åï¼ˆå»æ‰ç‰ˆæœ¬å·ï¼‰
                    package = re.split(r'[>=<!=]', line)[0].strip()
                    declared_packages.add(package)
        
        # æ˜ å°„å®é™…ä½¿ç”¨çš„åŒ…åˆ°requirementsåç§°
        used_packages = set()
        for imp in self.third_party_imports:
            if imp in self.package_mapping:
                used_packages.add(self.package_mapping[imp])
            else:
                used_packages.add(imp)
        
        # åˆ†æå·®å¼‚
        unused_declared = declared_packages - used_packages
        undeclared_used = used_packages - declared_packages
        
        return {
            'declared_packages': sorted(declared_packages),
            'used_packages': sorted(used_packages),
            'unused_declared': sorted(unused_declared),
            'undeclared_used': sorted(undeclared_used),
            'consistent': len(unused_declared) == 0 and len(undeclared_used) == 0
        }
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_project()
        consistency = self.check_requirements_consistency()
        
        report = []
        report.append("=" * 60)
        report.append("é¡¹ç›®ä¾èµ–åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        report.append(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        report.append(f"  - åˆ†ææ–‡ä»¶æ•°: {analysis['files_analyzed']}")
        report.append(f"  - æ€»å¯¼å…¥æ•°: {analysis['total_imports']}")
        report.append(f"  - ç¬¬ä¸‰æ–¹åº“: {len(analysis['third_party'])}")
        report.append(f"  - æ ‡å‡†åº“: {len(analysis['standard'])}")
        report.append(f"  - æœ¬åœ°æ¨¡å—: {len(analysis['local'])}")
        
        report.append(f"\nğŸ“¦ ç¬¬ä¸‰æ–¹åº“ä½¿ç”¨æƒ…å†µ:")
        for package, count in sorted(analysis['third_party'].items(), key=lambda x: x[1], reverse=True):
            report.append(f"  - {package}: {count} æ¬¡")
        
        report.append(f"\nğŸ”§ requirements.txt ä¸€è‡´æ€§æ£€æŸ¥:")
        if 'error' in consistency:
            report.append(f"  âŒ {consistency['error']}")
        else:
            if consistency['consistent']:
                report.append("  âœ… requirements.txt ä¸å®é™…ä½¿ç”¨ä¸€è‡´")
            else:
                if consistency['unused_declared']:
                    report.append("  âš ï¸  å£°æ˜ä½†æœªä½¿ç”¨çš„åŒ…:")
                    for pkg in consistency['unused_declared']:
                        report.append(f"    - {pkg}")
                
                if consistency['undeclared_used']:
                    report.append("  âŒ ä½¿ç”¨ä½†æœªå£°æ˜çš„åŒ…:")
                    for pkg in consistency['undeclared_used']:
                        report.append(f"    - {pkg}")
        
        report.append(f"\nğŸ—ï¸  å¸¸ç”¨æ ‡å‡†åº“:")
        top_standard = sorted(analysis['standard'].items(), key=lambda x: x[1], reverse=True)[:10]
        for lib, count in top_standard:
            report.append(f"  - {lib}: {count} æ¬¡")
        
        return '\n'.join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ä¾èµ–åˆ†æ...")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")

    analyzer = DependencyAnalyzer(project_root)
    print("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report = analyzer.generate_report()
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    report_file = project_root / 'tests' / 'dependency_analysis_report.txt'
    print(f"ä¿å­˜æŠ¥å‘Šåˆ°: {report_file}")
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


if __name__ == "__main__":
    main()
