# -*- coding: utf-8 -*-
"""
IO密集型任务模块

提供基于QThreadPool的IO任务处理框架：
- QRunnable任务基类，包含WorkerSignals(ok, err)
- 示例任务：HTTP拉取、磁盘读取等
- 异常统一回传err信号
- 全局QThreadPool复用，提供submit_io工具函数
"""

import os
import threading  # 保持活动任务强引用，避免被GC
import json
import time
import uuid
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Callable, Optional, Dict, List
from pathlib import Path

import requests
import cv2
import numpy as np
from PySide6 import QtCore
from PySide6.QtCore import QRunnable, QObject, Signal, QThreadPool, QTimer

from auto_approve.logger_manager import get_logger
from storage import save_image_blob, get_db_path, init_db


class WorkerSignals(QObject):
    """工作线程信号类"""
    # 成功信号：(task_id, result)
    ok = Signal(str, object)
    # 错误信号：(task_id, error_message, exception)
    err = Signal(str, str, object)
    # 进度信号：(task_id, progress_percent, message)
    progress = Signal(str, int, str)


class IOTaskBase(QRunnable):
    """IO任务基类
    
    所有IO密集型任务的基类，提供统一的信号处理和异常捕获
    """
    
    def __init__(self, task_id: str = None):
        super().__init__()
        # 使用UUID确保在高并发下任务ID绝对唯一，避免信号/注册表冲突
        self.task_id = task_id or f"io_task_{uuid.uuid4().hex[:12]}"
        self.signals = WorkerSignals()
        self._logger = get_logger()
        
    def run(self):
        """QRunnable接口实现，子类应重写execute方法"""
        try:
            result = self.execute()
            self.signals.ok.emit(self.task_id, result)
        except Exception as e:
            error_msg = f"IO任务执行失败: {str(e)}"
            self._logger.error(f"[{self.task_id}] {error_msg}")
            self._logger.debug(f"[{self.task_id}] 异常详情: {traceback.format_exc()}")
            self.signals.err.emit(self.task_id, error_msg, e)
    
    def execute(self) -> Any:
        """子类需要实现的具体执行逻辑"""
        raise NotImplementedError("子类必须实现execute方法")
    
    def emit_progress(self, progress: int, message: str = ""):
        """发射进度信号"""
        self.signals.progress.emit(self.task_id, progress, message)


class FileReadTask(IOTaskBase):
    """文件读取任务"""
    
    def __init__(self, file_path: str, encoding: str = 'utf-8', task_id: str = None):
        super().__init__(task_id)
        self.file_path = file_path
        self.encoding = encoding
    
    def execute(self) -> Dict[str, Any]:
        """执行文件读取"""
        self.emit_progress(10, f"开始读取文件: {self.file_path}")
        
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"文件不存在: {self.file_path}")
        
        self.emit_progress(30, "检查文件权限")
        
        # 根据文件扩展名选择读取方式
        file_ext = Path(self.file_path).suffix.lower()
        
        if file_ext == '.json':
            self.emit_progress(50, "读取JSON文件")
            with open(self.file_path, 'r', encoding=self.encoding) as f:
                content = json.load(f)
        elif file_ext in ['.png', '.jpg', '.jpeg', '.bmp']:
            self.emit_progress(50, "读取图像文件")
            # 使用cv2.imdecode处理中文路径
            img_data = np.fromfile(self.file_path, dtype=np.uint8)
            content = cv2.imdecode(img_data, cv2.IMREAD_UNCHANGED)
            if content is None:
                raise ValueError(f"无法解码图像文件: {self.file_path}")
        else:
            self.emit_progress(50, "读取文本文件")
            with open(self.file_path, 'r', encoding=self.encoding) as f:
                content = f.read()
        
        self.emit_progress(100, "文件读取完成")
        
        return {
            'file_path': self.file_path,
            'content': content,
            'file_size': os.path.getsize(self.file_path),
            'file_type': file_ext
        }


class FileWriteTask(IOTaskBase):
    """文件写入任务"""
    
    def __init__(self, file_path: str, content: Any, encoding: str = 'utf-8', task_id: str = None):
        super().__init__(task_id)
        self.file_path = file_path
        self.content = content
        self.encoding = encoding
    
    def execute(self) -> Dict[str, Any]:
        """执行文件写入"""
        self.emit_progress(10, f"开始写入文件: {self.file_path}")
        
        # 确保目录存在
        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
        
        self.emit_progress(30, "创建目录")
        
        file_ext = Path(self.file_path).suffix.lower()
        
        if file_ext == '.json':
            self.emit_progress(50, "写入JSON文件")
            with open(self.file_path, 'w', encoding=self.encoding) as f:
                json.dump(self.content, f, ensure_ascii=False, indent=2)
        elif file_ext in ['.png', '.jpg', '.jpeg', '.bmp']:
            self.emit_progress(50, "写入图像文件")
            if isinstance(self.content, np.ndarray):
                # 使用cv2.imencode处理中文路径
                success, encoded_img = cv2.imencode(file_ext, self.content)
                if success:
                    encoded_img.tofile(self.file_path)
                else:
                    raise ValueError(f"无法编码图像: {self.file_path}")
            else:
                raise TypeError("图像文件需要numpy.ndarray类型的内容")
        else:
            self.emit_progress(50, "写入文本文件")
            with open(self.file_path, 'w', encoding=self.encoding) as f:
                f.write(str(self.content))
        
        self.emit_progress(100, "文件写入完成")
        
        return {
            'file_path': self.file_path,
            'file_size': os.path.getsize(self.file_path),
            'success': True
        }


class HTTPRequestTask(IOTaskBase):
    """HTTP请求任务"""
    
    def __init__(self, url: str, method: str = 'GET', headers: Dict = None, 
                 data: Any = None, timeout: int = 30, task_id: str = None):
        super().__init__(task_id)
        self.url = url
        self.method = method.upper()
        self.headers = headers or {}
        self.data = data
        self.timeout = timeout
    
    def execute(self) -> Dict[str, Any]:
        """执行HTTP请求"""
        self.emit_progress(10, f"准备{self.method}请求: {self.url}")
        
        # 设置默认User-Agent
        if 'User-Agent' not in self.headers:
            self.headers['User-Agent'] = 'Autoclick/1.0'
        
        self.emit_progress(30, "发送HTTP请求")
        
        response = requests.request(
            method=self.method,
            url=self.url,
            headers=self.headers,
            data=self.data,
            timeout=self.timeout
        )
        
        self.emit_progress(70, f"收到响应: {response.status_code}")
        
        # 检查响应状态
        response.raise_for_status()
        
        self.emit_progress(90, "解析响应内容")
        
        # 尝试解析JSON，失败则返回文本
        try:
            content = response.json()
            content_type = 'json'
        except:
            content = response.text
            content_type = 'text'
        
        self.emit_progress(100, "HTTP请求完成")
        
        return {
            'url': self.url,
            'status_code': response.status_code,
            'headers': dict(response.headers),
            'content': content,
            'content_type': content_type,
            'encoding': response.encoding
        }


class BatchFileLoadTask(IOTaskBase):
    """批量文件加载任务"""
    
    def __init__(self, file_paths: List[str], task_id: str = None):
        super().__init__(task_id)
        self.file_paths = file_paths

    def _get_batch_worker_count(self, total_files: int) -> int:
        """根据任务规模计算受控并发度，避免线程无上限膨胀。"""
        if total_files <= 1:
            return 1
        cpu_count = os.cpu_count() or 4
        return max(2, min(total_files, cpu_count * 2, 8))

    def _load_single_file(self, file_path: str) -> Dict[str, Any]:
        """复用单文件读取逻辑，保证批量与单文件行为一致。"""
        return FileReadTask(file_path=file_path).execute()
    
    def execute(self) -> Dict[str, Any]:
        """执行批量文件加载"""
        results = {}
        failed_files = []
        total_files = len(self.file_paths)

        if total_files == 0:
            self.emit_progress(100, "批量加载完成，文件列表为空")
            return {
                'successful_files': results,
                'failed_files': failed_files,
                'total_count': 0,
                'success_count': 0,
                'failed_count': 0
            }

        max_workers = self._get_batch_worker_count(total_files)
        self.emit_progress(0, f"开始批量加载，共 {total_files} 个文件，并发线程: {max_workers}")

        completed_count = 0
        # 通过受控并发读取文件，避免在单线程内串行阻塞。
        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="batch-file-load") as executor:
            future_to_path = {
                executor.submit(self._load_single_file, file_path): file_path
                for file_path in self.file_paths
            }

            for future in as_completed(future_to_path):
                file_path = future_to_path[future]
                try:
                    result = future.result()
                    results[file_path] = result
                except Exception as e:
                    self._logger.warning(f"加载文件失败: {file_path}, 错误: {e}")
                    failed_files.append({'file_path': file_path, 'error': str(e)})

                completed_count += 1
                progress = int((completed_count / total_files) * 100)
                self.emit_progress(
                    progress,
                    f"已完成 {completed_count}/{total_files}: {os.path.basename(file_path)}"
                )

        self.emit_progress(100, f"批量加载完成，成功: {len(results)}, 失败: {len(failed_files)}")
        
        return {
            'successful_files': results,
            'failed_files': failed_files,
            'total_count': total_files,
            'success_count': len(results),
            'failed_count': len(failed_files)
        }


# 全局线程池实例
_global_thread_pool: Optional[QThreadPool] = None
_active_tasks_lock = threading.Lock()
# 使用字典保存活动任务，保证在任务运行期间不会被GC提前回收
_active_tasks: Dict[str, IOTaskBase] = {}


def _get_default_io_thread_count() -> int:
    """计算IO线程池默认线程数。

    说明：使用独立线程池时保守预留主线程资源，避免过度并发反压UI事件循环。
    """
    cpu_count = os.cpu_count() or 4
    return max(4, min(12, cpu_count + 2))


def get_global_thread_pool() -> QThreadPool:
    """获取全局线程池实例"""
    global _global_thread_pool
    if _global_thread_pool is None:
        # 使用专用QThreadPool，避免改写Qt全局线程池影响GUI内部任务稳定性。
        _global_thread_pool = QThreadPool()
        max_threads = _get_default_io_thread_count()
        _global_thread_pool.setMaxThreadCount(max_threads)
        # 线程空闲后尽快回收，降低长期运行的资源占用。
        _global_thread_pool.setExpiryTimeout(15000)
        get_logger().info(f"初始化全局IO线程池，最大线程数: {max_threads}")
    return _global_thread_pool


def submit_io(task: IOTaskBase, 
              on_success: Callable[[str, Any], None] = None,
              on_error: Callable[[str, str, Exception], None] = None,
              on_progress: Callable[[str, int, str], None] = None) -> str:
    """提交IO任务到线程池
    
    Args:
        task: IO任务实例
        on_success: 成功回调函数 (task_id, result)
        on_error: 错误回调函数 (task_id, error_message, exception)
        on_progress: 进度回调函数 (task_id, progress_percent, message)
    
    Returns:
        str: 任务ID
    """
    thread_pool = get_global_thread_pool()
    
    # 注册到活动任务表，避免被GC
    with _active_tasks_lock:
        _active_tasks[task.task_id] = task

    # 包装回调：先执行用户回调，再进行清理
    def _wrapped_ok(task_id: str, result: Any):
        try:
            if on_success:
                on_success(task_id, result)
        finally:
            with _active_tasks_lock:
                _active_tasks.pop(task_id, None)

    def _wrapped_err(task_id: str, error_message: str, exception: Exception):
        try:
            if on_error:
                on_error(task_id, error_message, exception)
        finally:
            with _active_tasks_lock:
                _active_tasks.pop(task_id, None)

    # 连接信号（使用包装器确保清理）
    task.signals.ok.connect(_wrapped_ok)
    task.signals.err.connect(_wrapped_err)
    if on_progress:
        task.signals.progress.connect(on_progress)
    
    # 提交任务
    thread_pool.start(task)
    
    get_logger().debug(f"提交IO任务: {task.task_id}, 当前活跃线程: {thread_pool.activeThreadCount()}")
    
    return task.task_id


def submit_file_read(file_path: str,
                     on_success: Callable[[str, Any], None] = None,
                     on_error: Callable[[str, str, Exception], None] = None,
                     encoding: str = 'utf-8',
                     task_id: str = None) -> str:
    """便捷函数：提交文件读取任务"""
    task = FileReadTask(file_path, encoding, task_id)
    return submit_io(task, on_success, on_error)


def submit_file_write(file_path: str, content: Any,
                      on_success: Callable[[str, Any], None] = None,
                      on_error: Callable[[str, str, Exception], None] = None,
                      encoding: str = 'utf-8',
                      task_id: str = None) -> str:
    """便捷函数：提交文件写入任务"""
    task = FileWriteTask(file_path, content, encoding, task_id)
    return submit_io(task, on_success, on_error)


def submit_http_request(url: str, method: str = 'GET',
                        on_success: Callable[[str, Any], None] = None,
                        on_error: Callable[[str, str, Exception], None] = None,
                        headers: Dict = None, data: Any = None,
                        timeout: int = 30, task_id: str = None) -> str:
    """便捷函数：提交HTTP请求任务"""
    task = HTTPRequestTask(url, method, headers, data, timeout, task_id)
    return submit_io(task, on_success, on_error)


def get_thread_pool_stats() -> Dict[str, int]:
    """获取线程池统计信息"""
    pool = get_global_thread_pool()
    return {
        'max_thread_count': pool.maxThreadCount(),
        'active_thread_count': pool.activeThreadCount(),
        'expiry_timeout': pool.expiryTimeout()
    }


# ==================== 扩展的IO任务类型 ====================

class DatabaseTask(IOTaskBase):
    """数据库操作任务（示例）"""

    def __init__(self, query: str, params: tuple = (), task_id: str = None):
        super().__init__(task_id)
        self.query = query
        self.params = params

    def execute(self) -> Dict[str, Any]:
        """执行数据库查询（这里是模拟实现）"""
        self.emit_progress(20, "连接数据库")
        time.sleep(0.1)  # 模拟连接延迟

        self.emit_progress(50, "执行查询")
        time.sleep(0.2)  # 模拟查询延迟

        self.emit_progress(80, "处理结果")

        # 模拟查询结果
        result = {
            'query': self.query,
            'params': self.params,
            'rows_affected': 5,
            'execution_time': 0.3,
            'timestamp': time.time()
        }

        self.emit_progress(100, "查询完成")
        return result


class WebSocketTask(IOTaskBase):
    """WebSocket连接任务"""

    def __init__(self, url: str, message: str = None, timeout: int = 30, task_id: str = None):
        super().__init__(task_id)
        self.url = url
        self.message = message
        self.timeout = timeout

    def execute(self) -> Dict[str, Any]:
        """执行WebSocket连接（模拟实现）"""
        self.emit_progress(10, f"连接WebSocket: {self.url}")

        # 这里应该使用真实的WebSocket库，如websocket-client
        # 为了避免额外依赖，这里只是模拟
        time.sleep(0.5)  # 模拟连接时间

        self.emit_progress(50, "连接已建立")

        if self.message:
            self.emit_progress(70, "发送消息")
            time.sleep(0.2)  # 模拟发送时间

        self.emit_progress(90, "接收响应")
        time.sleep(0.1)  # 模拟接收时间

        result = {
            'url': self.url,
            'connected': True,
            'message_sent': self.message,
            'response': f"Echo: {self.message}" if self.message else None,
            'timestamp': time.time()
        }

        self.emit_progress(100, "WebSocket任务完成")
        return result


class ConfigurationTask(IOTaskBase):
    """配置文件操作任务"""

    def __init__(self, config_path: str, operation: str = 'read',
                 config_data: Dict = None, task_id: str = None):
        super().__init__(task_id)
        self.config_path = config_path
        self.operation = operation  # 'read', 'write', 'update'
        self.config_data = config_data or {}

    def execute(self) -> Dict[str, Any]:
        """执行配置文件操作"""
        config_path = Path(self.config_path)

        if self.operation == 'read':
            self.emit_progress(30, f"读取配置文件: {config_path.name}")

            if not config_path.exists():
                raise FileNotFoundError(f"配置文件不存在: {config_path}")

            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.suffix.lower() == '.json':
                    config_data = json.load(f)
                else:
                    config_data = f.read()

            self.emit_progress(100, "配置读取完成")
            return {
                'operation': 'read',
                'config_path': str(config_path),
                'config_data': config_data,
                'file_size': config_path.stat().st_size
            }

        elif self.operation == 'write':
            self.emit_progress(30, f"写入配置文件: {config_path.name}")

            # 确保目录存在
            config_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_path, 'w', encoding='utf-8') as f:
                if config_path.suffix.lower() == '.json':
                    json.dump(self.config_data, f, indent=2, ensure_ascii=False)
                else:
                    f.write(str(self.config_data))

            self.emit_progress(100, "配置写入完成")
            return {
                'operation': 'write',
                'config_path': str(config_path),
                'bytes_written': config_path.stat().st_size
            }

        elif self.operation == 'update':
            self.emit_progress(20, f"更新配置文件: {config_path.name}")

            # 先读取现有配置
            existing_config = {}
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    if config_path.suffix.lower() == '.json':
                        existing_config = json.load(f)

            self.emit_progress(60, "合并配置数据")

            # 合并配置
            if isinstance(existing_config, dict) and isinstance(self.config_data, dict):
                existing_config.update(self.config_data)
                merged_config = existing_config
            else:
                merged_config = self.config_data

            # 写入更新后的配置
            config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(config_path, 'w', encoding='utf-8') as f:
                if config_path.suffix.lower() == '.json':
                    json.dump(merged_config, f, indent=2, ensure_ascii=False)
                else:
                    f.write(str(merged_config))

            self.emit_progress(100, "配置更新完成")
            return {
                'operation': 'update',
                'config_path': str(config_path),
                'merged_config': merged_config,
                'bytes_written': config_path.stat().st_size
            }

        else:
            raise ValueError(f"不支持的操作类型: {self.operation}")


class LogAnalysisTask(IOTaskBase):
    """日志分析任务"""

    def __init__(self, log_path: str, pattern: str = None,
                 max_lines: int = 10000, task_id: str = None):
        super().__init__(task_id)
        self.log_path = log_path
        self.pattern = pattern
        self.max_lines = max_lines

    def execute(self) -> Dict[str, Any]:
        """执行日志分析"""
        import re

        log_path = Path(self.log_path)
        if not log_path.exists():
            raise FileNotFoundError(f"日志文件不存在: {log_path}")

        self.emit_progress(10, f"开始分析日志: {log_path.name}")

        total_lines = 0
        matched_lines = []
        error_count = 0
        warning_count = 0

        pattern_regex = re.compile(self.pattern) if self.pattern else None

        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                if line_num > self.max_lines:
                    break

                total_lines += 1
                line = line.strip()

                # 统计错误和警告
                if 'ERROR' in line.upper() or 'FATAL' in line.upper():
                    error_count += 1
                elif 'WARNING' in line.upper() or 'WARN' in line.upper():
                    warning_count += 1

                # 模式匹配
                if pattern_regex and pattern_regex.search(line):
                    matched_lines.append({
                        'line_number': line_num,
                        'content': line
                    })

                # 更新进度
                if line_num % 1000 == 0:
                    progress = min(90, int((line_num / self.max_lines) * 90))
                    self.emit_progress(progress, f"已分析 {line_num} 行")

        self.emit_progress(100, "日志分析完成")

        return {
            'log_path': str(log_path),
            'total_lines': total_lines,
            'error_count': error_count,
            'warning_count': warning_count,
            'matched_lines': matched_lines[:100],  # 最多返回100个匹配行
            'pattern': self.pattern,
            'file_size': log_path.stat().st_size
        }


# ==================== 扩展的便捷函数 ====================

def submit_database_query(query: str, params: tuple = (),
                         on_success: Callable[[str, Any], None] = None,
                         on_error: Callable[[str, str, Exception], None] = None,
                         task_id: str = None) -> str:
    """便捷函数：提交数据库查询任务"""
    task = DatabaseTask(query, params, task_id)
    return submit_io(task, on_success, on_error)


def submit_websocket_task(url: str, message: str = None,
                         on_success: Callable[[str, Any], None] = None,
                         on_error: Callable[[str, str, Exception], None] = None,
                         timeout: int = 30, task_id: str = None) -> str:
    """便捷函数：提交WebSocket任务"""
    task = WebSocketTask(url, message, timeout, task_id)
    return submit_io(task, on_success, on_error)


def submit_config_operation(config_path: str, operation: str = 'read',
                           config_data: Dict = None,
                           on_success: Callable[[str, Any], None] = None,
                           on_error: Callable[[str, str, Exception], None] = None,
                           task_id: str = None) -> str:
    """便捷函数：提交配置文件操作任务"""
    task = ConfigurationTask(config_path, operation, config_data, task_id)
    return submit_io(task, on_success, on_error)


def submit_log_analysis(log_path: str, pattern: str = None, max_lines: int = 10000,
                       on_success: Callable[[str, Any], None] = None,
                       on_error: Callable[[str, str, Exception], None] = None,
                       task_id: str = None) -> str:
    """便捷函数：提交日志分析任务"""
    task = LogAnalysisTask(log_path, pattern, max_lines, task_id)
    return submit_io(task, on_success, on_error)


def optimize_thread_pool(cpu_intensive_ratio: float = 0.3, gui_priority: bool = True):
    """优化线程池配置 - 增强GUI响应性

    Args:
        cpu_intensive_ratio: CPU密集型任务比例，用于调整线程数
        gui_priority: 是否优先保证GUI响应性
    """
    pool = get_global_thread_pool()

    # 获取系统信息
    import os
    cpu_count = os.cpu_count() or 4

    # 根据任务类型调整线程数
    # 为了保证GUI响应性，适当减少线程数量
    if gui_priority:
        # GUI优先模式：保守的线程配置
        if cpu_intensive_ratio < 0.2:
            # 主要是IO任务，但为GUI保留资源
            optimal_threads = min(cpu_count * 2, 12)
        elif cpu_intensive_ratio > 0.7:
            # 主要是CPU任务，最保守
            optimal_threads = max(2, min(cpu_count, 4))
        else:
            # 混合任务，更保守
            optimal_threads = min(cpu_count + 2, 8)
    else:
        # 原有逻辑：性能优先
        if cpu_intensive_ratio < 0.2:
            optimal_threads = min(32, cpu_count * 4)
        elif cpu_intensive_ratio > 0.7:
            optimal_threads = max(2, cpu_count)
        else:
            optimal_threads = min(16, cpu_count * 2)

    # 设置线程池参数
    pool.setMaxThreadCount(optimal_threads)
    # 缩短超时时间，快速释放空闲线程
    pool.setExpiryTimeout(15000)  # 15秒超时，减少资源占用

    get_logger().info(f"线程池已优化，最大线程数: {optimal_threads} (CPU核心数: {cpu_count}, GUI优先: {gui_priority})")

    return {
        'max_threads': optimal_threads,
        'cpu_count': cpu_count,
        'cpu_intensive_ratio': cpu_intensive_ratio,
        'gui_priority': gui_priority
    }


def cleanup_thread_pool():
    """清理线程池资源"""
    global _global_thread_pool
    if _global_thread_pool:
        _global_thread_pool.waitForDone(5000)  # 等待5秒
        active_count = _global_thread_pool.activeThreadCount()
        if active_count > 0:
            get_logger().warning(f"线程池清理时仍有 {active_count} 个活跃线程")
        # 清理活动任务注册表（剩余的任务对象做安全释放）
        with _active_tasks_lock:
            _active_tasks.clear()
        _global_thread_pool = None
        get_logger().info("线程池已清理")


def get_detailed_thread_pool_stats() -> Dict[str, Any]:
    """获取详细的线程池统计信息"""
    pool = get_global_thread_pool()

    return {
        'max_thread_count': pool.maxThreadCount(),
        'active_thread_count': pool.activeThreadCount(),
        'expiry_timeout': pool.expiryTimeout(),
        'reserved_thread_count': getattr(pool, 'reservedThreadCount', lambda: 0)(),
        'stack_size': pool.stackSize(),
        'thread_priority': pool.threadPriority()
    }


class ImageSaveTask(IOTaskBase):
    """图像保存任务 - 仅保存到SQLite（BLOB），不再写入磁盘文件"""

    def __init__(self, image_data: np.ndarray, file_path: str,
                 quality: int = 95, task_id: str = None):
        super().__init__(task_id)
        self.image_data = image_data
        # 兼容旧接口：仍接受file_path，但仅用于生成name
        self.file_path = file_path
        self.quality = quality

    def execute(self) -> Dict[str, Any]:
        """执行图像保存到数据库的操作"""
        self.emit_progress(20, "准备编码图像")

        file_path = Path(self.file_path)
        name = file_path.name or f"image_{int(time.time())}.png"
        ext = file_path.suffix.lower() or ".png"

        # 编码为指定格式字节
        params = []
        if ext in (".jpg", ".jpeg"):
            params = [int(cv2.IMWRITE_JPEG_QUALITY), self.quality]
        elif ext == ".png":
            params = [int(cv2.IMWRITE_PNG_COMPRESSION), 6]

        ok, buf = cv2.imencode(ext if ext in (".jpg", ".jpeg", ".png", ".bmp") else ".png", self.image_data, params)
        if not ok:
            raise IOError("图像编码失败")

        self.emit_progress(60, "写入数据库")
        h, w = self.image_data.shape[:2]
        init_db()  # 确保数据库已初始化
        rowid = save_image_blob(name, buf.tobytes(), category="export", size=(w, h))

        self.emit_progress(100, "保存完成(数据库)")

        # 返回兼容字段，其中file_path使用db://前缀表示保存在数据库
        return {
            'file_path': f'db://export/{name}',
            'file_size': len(buf),
            'dimensions': (w, h),
            'quality': self.quality,
            'timestamp': time.time(),
            'image_id': rowid,
            'category': 'export'
        }


def submit_image_save(image_data: np.ndarray, file_path: str,
                     quality: int = 95,
                     on_success: Callable[[str, Any], None] = None,
                     on_error: Callable[[str, str, Exception], None] = None,
                     task_id: str = None) -> str:
    """便捷函数：提交图像保存任务
    
    Args:
        image_data: 图像数据 (numpy数组)
        file_path: 保存文件路径
        quality: 图像质量 (1-100, 仅对JPEG有效)
        on_success: 成功回调函数 (task_id, result)
        on_error: 错误回调函数 (task_id, error_message, exception)
        task_id: 任务ID (可选)
    
    Returns:
        str: 任务ID
    """
    task = ImageSaveTask(image_data, file_path, quality, task_id)
    return submit_io(task, on_success, on_error)
